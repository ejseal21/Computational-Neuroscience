{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR NAMES HERE**\n",
    "\n",
    "Spring 2020\n",
    "\n",
    "CS 443: Computational Neuroscience\n",
    "\n",
    "Project 4: Motion estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(['seaborn-colorblind', 'seaborn-darkgrid'])\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=3)\n",
    "\n",
    "# Automatically reload external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4) Build the deep network layers (Layers 3-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a. Layer 3: Short-range filter cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (i) Implement helper methods\n",
    "\n",
    "Layers 3+ do convolution to compute their `netIn`, so you will create a method that handles the construction of all the various kernels.\n",
    "\n",
    "- Implement `make_kernels` in `MotionNet`. This makes all the excitatory/inhibitory convolutional kernels in Levels 3+. For now, just create the Level 3 excitatory anisotropic Gaussian kernels. The elongated direction should align with the preferred direction of each neuron. I suggest a filter size of (13, 13) or thereabouts (small relative to the spatial extent of the video). Remember to wrap code in level-specific boolean.\n",
    "- Call `make_kernels` in constructor to build all the kernels whenever you create a network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (ii) Implement and plot the layer output\n",
    "\n",
    "Here are the steps we will perform over and over again to add a new layer.\n",
    "\n",
    "- Implement `d_short_range_filter`.\n",
    "- Update `update_net` to integrate cell activity in Level 3 (remember to put your code in a boolean if statement).\n",
    "- Simulate the noise-free rightward motion RDK video (`test_rdk_0`) with network layers 1, 2, and 3.\n",
    "- Run the direction decode test below.\n",
    "- In the cell below, compute the rectifed layer output of the short-range filter (srf) cells.\n",
    "- Plot the activation of the rectified output of all directions using `plot_act_image_grid`. Compare for consistency to `right_test_level3_grid.mov`. \n",
    "    - Note that I am plotting starting at time step 9 to the end in steps of 10.\n",
    "- Plot the rectified srf cells using your vector sum plot. Compare for consistency with `right_test_level3_vectors.mov`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up input\n",
    "np.random.seed(0)\n",
    "\n",
    "# Define parameters\n",
    "\n",
    "# Simulate net\n",
    "\n",
    "\n",
    "# Compute layer 3 output activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your grid of image plots of the layer 3 cells output over time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: Decoded directional transient directions\n",
    "\n",
    "**IMPORTANT NOTE:** Your exact values **WILL** differ and thats ok (due to parameter choices and different implementation decisions). You simply want to get rough qualitative agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Your decoded evidence for each direction at the end of the simulation is\\n{net.decode_direction(net.srf_cells, -1)}\\nand should roughly look like\\n[112.433 106.568 102.521 102.601   0.5   105.73  105.342 107.313]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your vector sum plot of the layer 3 output over time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. Layer 4: Spatial and directional competition cells in MT\n",
    "\n",
    "Adding layer 4 follows the same procedure as before to add a new layer:\n",
    "\n",
    "- Update `make_kernels` to build Level 4 excitatory and inhibitory kernels. Kernel sizes should be between 1.5-2x those used in Layer 3, though remember we want to implement on-center/off-surround organization (excit filter ~same size as Layer 3, inhib filter a bit bigger with larger sigma).\n",
    "- Implement `d_competition_layer`.\n",
    "- Update `update_net` to integrate cell activity in Level 4 (remember to put your code in a boolean if statement).\n",
    "- Simulate the noise-free rightward motion RDK video (`test_rdk_0`) with network layers 1, 2, 3, and 4.\n",
    "- Run the direction decode test below.\n",
    "- In the cell below, compute the rectifed competition cell output.\n",
    "- Plot the activation of the rectified output of all directions using `plot_act_image_grid`. Compare for consistency to `right_test_level4_grid.mov`.\n",
    "    - Note that I am plotting starting at time step 9 to the end in steps of 10. (*Please ignore the drifting of the placement of the second row of plots*).\n",
    "- Plot the rectified competition cells using your vector sum plot. Compare for consistency with `right_test_level4_vectors.mov`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define meta parameters\n",
    "\n",
    "# Set up input\n",
    "np.random.seed(0)\n",
    "\n",
    "# Define parameters\n",
    "\n",
    "# Simulate net\n",
    "\n",
    "\n",
    "# Compute layer 4 output activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your grid of image plots of the layer 4 cells output over time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: Decoded directional transient directions\n",
    "\n",
    "**IMPORTANT NOTE:** Your exact values **WILL** differ and thats ok (due to parameter choices and different implementation decisions). You simply want to get rough qualitative agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Your decoded evidence for each direction at the end of the simulation is\\n{net.decode_direction(net.comp_cells, -1)}\\nand should roughly look like\\n[118.35   47.927  34.174  42.399   0.     48.013  38.499  46.949]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your vector sum plot of the layer 4 output over time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4e. Level 5: Long-range filter in MT\n",
    "\n",
    "Adding layer 5 follows the same procedure as before to add a new layer:\n",
    "\n",
    "- Update `make_kernels` to build Level 5 excitatory kernels. Filter size should be ~4x Layer 3 SRF (hence the name :)\n",
    "- Implement `d_long_range`.\n",
    "- Update `update_net` to integrate cell activity in Level 5 (remember to put your code in a boolean if statement).\n",
    "- Simulate the noise-free rightward motion RDK video (`test_rdk_0`) with network layers 1, 2, 3, 4, and 5.\n",
    "- Run the direction decode test below.\n",
    "- In the cell below, rectify the output of the long-range filter cells.\n",
    "- Plot the activation of the rectified output of all directions using `plot_act_image_grid`. Compare for consistency to `right_test_level5_grid.mov`.\n",
    "    - Note that I am plotting starting at time step 9 to the end in steps of 10. (*Please ignore the drifting of the placement of the second row of plots*).\n",
    "- Plot the rectified long-range filter cells using your vector sum plot. Compare for consistency with `right_test_level5_vectors.mov`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define meta parameters\n",
    "\n",
    "# Set up input\n",
    "np.random.seed(0)\n",
    "\n",
    "# Define parameters\n",
    "\n",
    "# Simulate net\n",
    "\n",
    "# Compute layer 5 output activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your grid of image plots of the layer 5 cells output over time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: Decoded directional transient directions\n",
    "\n",
    "**IMPORTANT NOTE:** Your exact values **WILL** differ and thats ok (due to parameter choices and different implementation decisions). You simply want to get rough qualitative agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Your decoded evidence for each direction at the end of the simulation is\\n{net.decode_direction(net.lr_cells, -1)}\\nand should roughly look like\\n[212.698  91.528  67.421  80.858   0.     90.174  73.545  89.044]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your vector sum plot of the layer 5 output over time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4f. Level 6: Direction grouping in MSTd\n",
    "\n",
    "Adding layer 6 follows the same procedure as before to add a new layer, **with the exception of implementing MSTd-MT feedback (Layer 6-> 5)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (i) Implement and test MSTd-MT feedback (Layer 6 -> Layer 5)\n",
    "\n",
    "- `make_mstd_fb_wts`. Weights for directional inhibition in MSTd and feedback from MSTd to MT (long-range filters). Each cell recieves no inhibition from other neurons with the same direction preference (0 wt). Each cell recieves the most inhibition from the opponent direction (2 wt). All other directions recieve equally weighted moderate inhibition (1 wt). **Run the test code below**.\n",
    "- Create your MSTd feedback weights in the constructor.\n",
    "- Implement `mstd_fb` **then run the test code below**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (ii) Test `make_mstd_fb_wts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.1\n",
    "n_dirs = 8\n",
    "\n",
    "test_net = MotionNet(dt,\n",
    "                     n_dirs,\n",
    "                     lvl1_params=None,\n",
    "                     lv1_hgate_params=None,\n",
    "                     do_lvl2=False,\n",
    "                     do_lvl3=False,\n",
    "                     do_lvl4=False,\n",
    "                     do_lvl5=False,\n",
    "                     do_lvl6=False)\n",
    "wts = test_net.make_mstd_fb_wts()\n",
    "print('Your MSTd inhibitory weight matrix is:\\n', wts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be:\n",
    "\n",
    "    [[0. 1. 1. 1. 2. 1. 1. 1.]\n",
    "     [1. 0. 1. 1. 1. 2. 1. 1.]\n",
    "     [1. 1. 0. 1. 1. 1. 2. 1.]\n",
    "     [1. 1. 1. 0. 1. 1. 1. 2.]\n",
    "     [2. 1. 1. 1. 0. 1. 1. 1.]\n",
    "     [1. 2. 1. 1. 1. 0. 1. 1.]\n",
    "     [1. 1. 2. 1. 1. 1. 0. 1.]\n",
    "     [1. 1. 1. 2. 1. 1. 1. 0.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iii) Test `mstd_fb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.1\n",
    "n_dirs = 8\n",
    "n_frames = 1\n",
    "\n",
    "net = MotionNet(dt=dt,\n",
    "                n_dirs=n_dirs,\n",
    "                lvl1_params=None,\n",
    "                lv1_hgate_params=None,\n",
    "                lvl2_inter_params=None,\n",
    "                lvl2_params=None,\n",
    "                lvl3_params=None,\n",
    "                lvl3_excit_ker_params=None,\n",
    "                lvl4_params=None,\n",
    "                lvl4_excit_ker_params=None,\n",
    "                lvl4_inhib_ker_params=None,\n",
    "                lvl5_params=None,\n",
    "                lvl5_excit_ker_params=None,\n",
    "                lvl6_params=None,\n",
    "                lvl6_inhib_ker_params=None,\n",
    "                do_lvl2=False,\n",
    "                do_lvl3=False,\n",
    "                do_lvl4=False,\n",
    "                do_lvl5=False,\n",
    "                do_lvl6=False\n",
    "                )\n",
    "net.mstd_inhib_ker = np.array([[0, 1, 0], [1, 2, 1], [0, 1, 0]])\n",
    "\n",
    "test_act = np.random.random(size=(8, 4, 4))\n",
    "test_fb = net.mstd_fb(t=1, curr_mstd_out=test_act)\n",
    "print('Your feedback signal from MSTd (1st 2 directions) is:\\n', test_fb[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get the following output:\n",
    "\n",
    "    Your feedback signal from MSTd (1st 2 directions) is:\n",
    "     [[[12.499 17.705 19.242 15.673]\n",
    "      [17.464 21.184 22.453 19.604]\n",
    "      [21.062 22.546 23.542 20.478]\n",
    "      [18.182 19.925 21.551 18.902]]\n",
    "\n",
    "     [[12.181 19.097 20.065 15.029]\n",
    "      [17.794 25.019 24.393 18.994]\n",
    "      [21.355 24.216 24.465 18.923]\n",
    "      [16.833 18.432 18.755 16.604]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iv) Implement Layer 6 (MSTd)\n",
    "\n",
    "This now follows the usual workflow:\n",
    "\n",
    "- Update `make_kernels` to build Level 6 inhibitory feedback kernel. Filter size should be same size as the long-range filter.\n",
    "- Update `d_long_range` to include this feedback signal if we are simulating Layer 6. **NOTE:** When computing the MSTd feedback signal, it should be based on the output signal at $t-1$. Why? `self.mstd_out[t]` should be undefined (we compute Level 5 before Level 6).\n",
    "- Implement `d_mstd_grouping` (Level 6 derivatives).\n",
    "- Update `update_net` to integrate cell activity in Level 6 (remember to put your code in a boolean if statement).\n",
    "- Simulate the noise-free rightward motion RDK video (`test_rdk_0`) with all network layers.\n",
    "- Run the direction decode test below.\n",
    "- In the cell below, rectify the MSTd cells.\n",
    "- Plot the activation of the rectified output of all directions using `plot_act_image_grid`. Compare for consistency to `right_test_level6_grid.mov`. Note that I am plotting starting at time step 9 to the end in steps of 10. (*Please ignore the drifting of the placement of the second row of plots*).\n",
    "- Plot the rectified MSTd cells using your vector sum plot. Compare for consistency with `right_test_level6_vectors.mov`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up input\n",
    "np.random.seed(0)\n",
    "\n",
    "# Define parameters\n",
    "\n",
    "# Simulate net\n",
    "\n",
    "# Compute rectified layer output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your grid of image plots of the layer 6 cells output over time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: Decoded directional transient directions\n",
    "\n",
    "**IMPORTANT NOTE:** Your exact values **WILL** differ and thats ok (due to parameter choices and different implementation decisions). You simply want to get rough qualitative agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Your decoded evidence for each direction at the end of the simulation is\\n{net.decode_direction(net.mstd_cells, -1)}\\nand should roughly look like\\n[402.962   0.149   0.      0.047   0.      0.453   0.17    0.027]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your vector sum plot of the layer 6 output over time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5) Direction discrimination with noise\n",
    "\n",
    "- Perform your own full network simulations to discriminate the coherent motion direction in videos with random noise (uncorrelated frame-to-frame) and correlated every 3 frames.\n",
    "- Show dominant direct decoded direction and make plots of each network stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a. Simulations with uncorrelated frame-to-frame noise.\n",
    "\n",
    "Generate new random dot kinematograms with noise (that you already implemented support for). Test out the full network with noise, visualize the estimated motion, and print out the decoded evidence for different motion directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. Simulations with complex noise: coherence videos.\n",
    "\n",
    "- Import the 10 videos with different levels of coherence (in `coherence_stimuli.mat`, which is in MATLAB format. Check out `loadmat` function). In these videos, three coherent motion videos are interleaved (e.g. frame 1, 4, 7, ... go together, 2, 5, 8, ... go together, etc.). The interleaving makes it challenging for the network to detect the percieved motion because dots don't correspond to each other frame-to-frame, yet we still percieve motion!\n",
    "- There are 10 levels of dot coherence (100% coherence = no noise, 0% coherence = all noise). Test out the network like in 5a in SOME of these videos (doesn't need to be all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Additional visualizations\n",
    "\n",
    "Neural network simulations with many areas and dimensions (space, motion direction, etc) present challenges to interpreting and visualizing the dynamics. Extensions like the following ideas are aimed to give us a better picture of the network's operation.\n",
    "\n",
    "#### Needle plot\n",
    "\n",
    "- Visualize the activity of ALL direction cells at ALL spatial positions with a needle plot. For each position (x, y), draw line segments (no arrowheads) coming out of (x,y) aligned each of the 8 preferred directions (e.g. for rightward motion, line sticks to the right, for up-and-right motion line sticks +45 deg, etc). The length is proportioonal to the cell activity. Do whatever normalization to make things look good/helpful. Make your plot show activations over time.\n",
    "\n",
    "#### Mean activity plots\n",
    "\n",
    "- Plot the mean direction activity across space for each area.\n",
    "\n",
    "#### Better vector sum plots\n",
    "\n",
    "- Superimpose the motion vectors onto of the input dot patterns so that you can see the correspondance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Additional analyses\n",
    "\n",
    "Analyze the network in new ways. Many ways to do this, here are some ideas:\n",
    "\n",
    "- Plot/interpret the motion direction evidence over time in different areas.\n",
    "- Compute the relative SNR (signal to noise ratio) in different areas for the dominant motion vs rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Additional RDKs\n",
    "\n",
    "- Analyze/simulate your network with RDKs with different movement directions, changing directions, two simultaneous directions (called transparent motion), etc...lots of possibilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Different motion detection mechanisms\n",
    "\n",
    "- We used nulling inhibition to detect motion. You should be able to \"swap in\" other motion detection mechanisms (for example, in easiest-to-hardest to implement order, Reichardt, Gradient, Spatio-temporal energy, etc).\n",
    "- For Reichardt (correlation-based), the easiest approach that tends to work well is to add the pre and post movement activations together in whatever the neuron's prefered direction is, then apply a threshold to filter out low matches. For example, to detect rightward motion, you could do something like $x_i(t-1) + x_{i+1}(t)$ then compare that to an activation threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Support for multiple speeds\n",
    "\n",
    "- (Challenging) This is more open ended. A simplification we made to keep the project scope reasonable is to only support cells that only detect 1 px/frame motion. Experiment, brainstorm, and implement support for other speeds (2 px/frame, 3 px/frame, etc)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
